{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning, the best n_neighbors of the K-Neighbors Classifier is 3, the best max_depth of the Decision Tree Classifier is 7.\n",
    "We can discover that the testing accuracy for the ensemble method: voting, bagging, and boosting are all 0.823."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import csv\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "file = open('D:/course/deep learning/bonus/train.csv', encoding='utf-8')\n",
    "reader = csv.reader(file)\n",
    "next(reader)\n",
    "X = np.ndarray((0, 2))\n",
    "y = np.ndarray((0,))\n",
    "y_mapping = {'Bob': 0, 'Kate': 1, 'Mark': 2, 'Sue': 3}\n",
    "i = 0\n",
    "for row in reader:\n",
    "    i += 1\n",
    "    X = np.vstack((X, np.array(row[0:2])))\n",
    "    y = np.append(y, y_mapping[row[2]])\n",
    "X = X.astype(np.float)\n",
    "y = y.astype(np.float)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold out testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "# hold out validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Outer fold 1/5]\n",
      "Test accuracy: 0.82 (n_neighbors=5 selected by inner 10-fold CV)\n",
      "[Outer fold 2/5]\n",
      "Test accuracy: 0.86 (n_neighbors=3 selected by inner 10-fold CV)\n",
      "[Outer fold 3/5]\n",
      "Test accuracy: 0.77 (n_neighbors=3 selected by inner 10-fold CV)\n",
      "[Outer fold 4/5]\n",
      "Test accuracy: 0.82 (n_neighbors=3 selected by inner 10-fold CV)\n",
      "[Outer fold 5/5]\n",
      "Test accuracy: 0.81 (n_neighbors=3 selected by inner 10-fold CV)\n",
      "\n",
      "Test accuracy: 0.81 (5x10 nested CV)\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV: tune the n_neighbors of the K-Neighbors Classifier\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "inner_cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "outer_scores = []\n",
    "# outer folds\n",
    "for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    print('[Outer fold %d/5]' % (i + 1))\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    pipe = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsClassifier()]])\n",
    "    # hyperparameter tuning by grid search CV\n",
    "    param_grid = {'clf__n_neighbors':list(range(1, 11))}\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=inner_cv)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_clf = gs.best_estimator_\n",
    "    best_clf.fit(X_train, y_train)    \n",
    "    outer_scores.append(best_clf.score(X_test, y_test))\n",
    "    print('Test accuracy: %.2f (n_neighbors=%d selected by inner 10-fold CV)' % \n",
    "                  (outer_scores[i], gs.best_params_['clf__n_neighbors']))\n",
    "\n",
    "print('\\nTest accuracy: %.2f (5x10 nested CV)' % np.mean(outer_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Outer fold 1/5]\n",
      "Test accuracy: 0.80 (max_depth=7 selected by inner 10-fold CV)\n",
      "[Outer fold 2/5]\n",
      "Test accuracy: 0.78 (max_depth=7 selected by inner 10-fold CV)\n",
      "[Outer fold 3/5]\n",
      "Test accuracy: 0.78 (max_depth=6 selected by inner 10-fold CV)\n",
      "[Outer fold 4/5]\n",
      "Test accuracy: 0.82 (max_depth=7 selected by inner 10-fold CV)\n",
      "[Outer fold 5/5]\n",
      "Test accuracy: 0.80 (max_depth=7 selected by inner 10-fold CV)\n",
      "\n",
      "Test accuracy: 0.80 (5x10 nested CV)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g1022\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV: tune the max_depth of the Decision Tree Classifier\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "inner_cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "outer_scores = []\n",
    "# outer folds\n",
    "for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    print('[Outer fold %d/5]' % (i + 1))\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    pipe = Pipeline([['sc', StandardScaler()], ['clf', DecisionTreeClassifier()]])\n",
    "    # hyperparameter tuning by grid search CV\n",
    "    param_grid = {'clf__max_depth':list(range(1, 11))}\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=inner_cv)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_clf = gs.best_estimator_\n",
    "    best_clf.fit(X_train, y_train)    \n",
    "    outer_scores.append(best_clf.score(X_test, y_test))\n",
    "    print('Test accuracy: %.2f (max_depth=%d selected by inner 10-fold CV)' % \n",
    "                  (outer_scores[i], gs.best_params_['clf__max_depth']))\n",
    "\n",
    "print('\\nTest accuracy: %.2f (5x10 nested CV)' % np.mean(outer_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] accuracy-train = 0.922, accuracy-test = 0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g1022\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ensemble method: voting\n",
    "pipe1 = Pipeline([['sc', StandardScaler()], ['clf', LogisticRegression(C = 10, random_state = 0, solver = \"liblinear\")]])\n",
    "pipe2 = Pipeline([['clf', DecisionTreeClassifier(max_depth = 7, random_state = 0)]])\n",
    "pipe3 = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsClassifier(n_neighbors = 3)]])\n",
    "\n",
    "clf = VotingClassifier(estimators=[('lr', pipe1), ('dt', pipe2), ('knn', pipe3)], voting='soft')\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "clf_train = accuracy_score(y_train, y_train_pred) \n",
    "clf_test = accuracy_score(y_test, y_test_pred) \n",
    "print('[Voting] accuracy-train = %.3f, accuracy-test = %.3f' % (clf_train, clf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bagging] accuracy-train = 0.904, accuracy-test = 0.823\n"
     ]
    }
   ],
   "source": [
    "# ensemble method: bagging\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=7, random_state=0)\n",
    "bag = BaggingClassifier(base_estimator=tree, n_estimators=1000, max_samples=0.7, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, n_jobs=1, random_state=1)\n",
    "\n",
    "# Bagging\n",
    "bag = bag.fit(X_train, y_train)\n",
    "y_train_pred = bag.predict(X_train)\n",
    "y_test_pred = bag.predict(X_test)\n",
    "\n",
    "bag_train = accuracy_score(y_train, y_train_pred) \n",
    "bag_test = accuracy_score(y_test, y_test_pred) \n",
    "print('[Bagging] accuracy-train = %.3f, accuracy-test = %.3f' % (bag_train, bag_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AdaBoost] accuracy-train = 1.000, accuracy-test = 0.823\n"
     ]
    }
   ],
   "source": [
    "# ensemble method: boosting\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=7)\n",
    "\n",
    "# adaboost\n",
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=1000)\n",
    "ada = ada.fit(X_train, y_train)\n",
    "y_train_pred = ada.predict(X_train)\n",
    "y_test_pred = ada.predict(X_test)\n",
    "\n",
    "ada_train = accuracy_score(y_train, y_train_pred)\n",
    "ada_test = accuracy_score(y_test, y_test_pred)\n",
    "print('[AdaBoost] accuracy-train = %.3f, accuracy-test = %.3f' % \n",
    "      (ada_train, ada_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
